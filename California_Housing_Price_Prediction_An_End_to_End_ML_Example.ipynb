{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-SvHPLOx5Bb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California housing dataset as a DataFrame\n",
        "# The dataset includes features related to California housing data, which are used as predictors for the target (housing prices).\n",
        "housing = fetch_california_housing(as_frame=True).frame\n",
        "\n",
        "# Define target and feature matrices\n",
        "# The target variable 'MedHouseVal' (median house value) is separated from the feature set.\n",
        "y = housing.pop(\"MedHouseVal\")  # Target variable (median house value)\n",
        "X = housing  # Feature variables\n",
        "\n",
        "# Feature Engineering: Adding new feature - Ratio of average bedrooms to average rooms\n",
        "# This engineered feature provides a normalized metric representing the density of bedrooms per room,\n",
        "# which may better capture housing density than using raw 'AveRooms' or 'AveBedrms'.\n",
        "X[\"AveBedrmsRatio\"] = X[\"AveBedrms\"] / X[\"AveRooms\"]\n",
        "X = X.drop(columns=['AveRooms', \"AveBedrms\"])  # Drop original columns to avoid multicollinearity\n",
        "\n",
        "# Clustering based on location features to group similar areas\n",
        "# Clustering the 'Latitude' and 'Longitude' features groups properties into regional clusters,\n",
        "# adding a categorical 'Location' feature to help capture potential location-based price trends.\n",
        "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(X[['Latitude', 'Longitude']])\n",
        "X['Location'] = kmeans.labels_  # Add cluster labels as a new feature\n",
        "X = X.drop(columns=['Latitude', 'Longitude'])  # Drop the latitude and longitude columns\n",
        "\n",
        "# Outlier Detection: Remove outliers using IsolationForest\n",
        "# Outliers can skew the model and lead to overfitting. We use IsolationForest to filter out outliers,\n",
        "# retaining only inlier data points, which represent typical housing data patterns.\n",
        "isolation_forest = IsolationForest(random_state=42)\n",
        "outlier_pred = isolation_forest.fit_predict(X)\n",
        "X, y = X[outlier_pred == 1], y[outlier_pred == 1]  # Keep only inliers\n",
        "\n",
        "# Train-test split\n",
        "# Splitting data into training and test sets ensures we can validate the model's generalizability.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a preprocessing and modeling pipeline\n",
        "# This pipeline standardizes features and applies linear regression as a model.\n",
        "# Pipelines ensure that transformations are consistently applied, reducing risk of data leakage.\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize features to mean 0, variance 1 for better model performance\n",
        "    ('estimator', LinearRegression())  # Linear regression as the predictive model\n",
        "])\n",
        "\n",
        "# Define grid search parameters for Linear Regression\n",
        "# Grid search allows us to explore multiple parameter settings efficiently.\n",
        "# Here we tune 'fit_intercept' and 'positive' to find the best configuration for linear regression.\n",
        "param_grid = {\n",
        "    'estimator__fit_intercept': [True, False],  # Fit intercept affects how the model handles the intercept term\n",
        "    'estimator__positive': [True, False]        # Positive constraint restricts coefficients to positive values\n",
        "}\n",
        "\n",
        "# Set up and perform Grid Search with cross-validation\n",
        "# GridSearchCV performs an exhaustive search over the parameter grid, optimizing for the lowest mean squared error.\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output best model parameters and test score\n",
        "# Reporting the best score and parameters gives insight into model performance and the optimal settings found.\n",
        "print(f\"Best score (cross-validated negative MSE): {grid_search.best_score_:.3f}\")\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate model on the test set\n",
        "# Test set performance reflects model generalizability. RÂ² score indicates the percentage of variance explained by the model.\n",
        "best_model = grid_search.best_estimator_\n",
        "test_score = best_model.score(X_test, y_test)\n",
        "print(f\"Test set R^2 score: {test_score:.3f}\")\n"
      ]
    }
  ]
}